# MapReduce: Simpli ed Data Processing on Large Clusters

## 摘要

MapReduce是用于处理和生成大型数据集的编程模型和相关的实现。 用户指定一个处理键/值对以生成一组中间键/值对的map函数，以及一个归并与同一中间键关联的所有中间值的reduce函数。 如本文所示，在此模型中可以表达许多现实世界中的任务。

以这种功能风格编写的程序会自动并行化，并在大型商用机器集群上执行。 运行时系统负责划分输入数据，安排程序在一组机器上的执行，处理机器故障以及管理所需的机器间通信的细节。 这使没有并行和分布式系统经验的程序员可以轻松利用大型分布式系统的资源。

我们对MapReduce的实现可在大型商用机器集群上运行，并且具有高度可扩展性：典型的MapReduce计算可在数千台机器上处理数TB的数据。 程序员发现该系统易于使用：每天执行数百个MapReduce程序，每天在Google的集群上执行多达一千个MapReduce作业。

## 1. 介绍

在过去的五年中，Google的作者和许多其他人已经实现了数百种特殊用途的计算，这些计算处理大量的原始数据（例如抓取的文档，Web请求日志等），以计算各种派生数据，例如作为倒排索引，Web文档的图形结构的各种表示形式，每个主机爬网的页面数摘要，给定一天中最频繁的查询集等。大多数此类计算在概念上都很简单。 但是，输入数据通常很大，并且必须在数百或数千台机器上分布计算，以便在合理的时间内完成操作。 如何并行化计算，分配数据和处理故障的问题共同困扰了使用大量复杂代码来掩盖最初的简单计算，以应对这些问题。

为了应对这种复杂性，我们设计了一个新的抽象，该抽象使我们能够表达我们试图执行的简单计算，但在库中隐藏了并行化，容错，数据分发和负载均衡的混乱细节。 我们的抽象受到Lisp和许多其他功能语言中存在的Map和Reduce原语的启发。 我们意识到，大多数计算都涉及对输入中的每个逻辑“记录”应用Map操作，以便计算一组中间键/值对，然后对共享同一键的所有值应用Reduce操作， 为了适当地组合得出的数据。 我们使用具有用户指定的Map和Reduce运算的功能模型，使我们能够轻松地并行进行大型计算，并将重新执行用作容错的主要机制。

这项工作的主要贡献是一个简单而强大的界面，该界面可实现大规模计算的自动并行化和分配，并结合了该界面的实现，可在大型商用PC集群上实现高性能。

第2节描述了基本的编程模型，并给出了一些示例。 第3节介绍了针对我们基于集群的计算环境量身定制的MapReduce接口的实现。 第4节描述了一些有用的编程模型改进。 第5节对我们执行各种任务的性能进行了度量。 第6节探讨了MapReduce在Google中的用法，包括我们使用它作为重写生产索引系统基础的经验。 第7节讨论相关和未来的工作。

## 2. 编程模型

该计算采用一组输入键/值对，并产生一组输出键/值对。 MapReduce库的用户将计算表示为两个函数：Map和Reduce。

由用户编写的Map接受一个输入对，并生成一组中间键/值对。 MapReduce库将与同一中间键I关联的所有中间值分组在一起，并将它们传递给Reduce函数。

还由用户编写的Reduce函数接受中间键I和该键的一组值。 它将这些值合并在一起以形成可能较小的一组值。 通常，每个Reduce调用仅产生零或一个输出值。 中间值通过迭代器提供给用户的Reduce函数。 这使我们能够处理太大而无法容纳在内存中的值列表。

## 2.1. Example

考虑对大量文档中每个单词的出现次数进行计数的问题。 用户将编写类似于以下伪代码的代码：

```js
map(String key, String value):
    // key: document name
    // value: document contents
    for each word w in value:
    EmitIntermediate(w, "1");
reduce(String key, Iterator values):
    // key: a word
    // values: a list of counts
    int result = 0;
    for each v in values:
    result += ParseInt(v);
    Emit(AsString(result));

```

Map函数发出每个单词以及相关的出现次数（在此简单示例中为“1”）。 减少功能将特定单词发出的所有计数加在一起。

另外，用户使用输入和输出文件的名称以及可选的调整参数在mapreduce规范对象中将代码写入ll。 然后，用户调用MapReduce函数，并将其传递给指定对象。 用户代码与MapReduce库（在C ++中实现）链接在一起。 附录A包含此示例的完整程序文本。

另外，用户编写代码以使用输入和输出文件的名称以及可选的调整参数来填充mapreduce规范对象。 然后，用户调用MapReduce函数，并将其传递给指定对象。 用户代码与MapReduce库（在C ++中实现）链接在一起。 附录A包含此示例的完整程序文本。

## 2.2. Types

即使先前的伪代码是根据字符串输入和输出编写的，但从概念上讲，用户提供的map和reduce函数具有关联的类型：

`map (k1,v1)          -> list(k2,v2)`

`reduce (k2,list(v2)) -> list(v2)`

即，输入键和值是从与输出键和值不同的域中提取的。 此外，中间键和值与输出键和值来自同一域。

我们的C ++实现在用户定义的函数之间传递字符串，并将其留给用户代码以在字符串和适当的类型之间进行转换。

## 2.3. More Examples

这是一些有趣的程序的简单示例，可以轻松地表达为MapReduce计算。

### 2.3.1 Distributed Grep

如果地图功能与提供的模式匹配，则会发出这一行。 Reduce函数是一个标识函数，它仅将提供的中间数据复制到输出中。

### 2.3.2 Count of URL Access Frequency

Map功能处理网页请求的日志并输出<URL，1>。 Reduce函数将同一URL的所有值加在一起，并发出<URL，total count>对。

### 2.3.3 Reverse Web-Link Graph

Map函数将每个链接的<target，source>对输出到在名为source的页面中找到的目标URL。 Reduce函数将与给定目标URL关联的所有源URL的列表连接起来，并发出对<target，list（source>。

### 2.3.4 Term-Vector per Host

Term-Vector将在一个文档或一组文档中出现的最重要的单词概括为<word, frequency>对的列表。 Map函数为每个输入文档发出一个<hostname, term vector>对（其中主机名是从文档的URL中提取的）。 将减少功能传递给给定主机的所有每个文档术语向量。 它将这些术语向量相加，丢弃不常用的术语，然后发出最后的<hostname,  term vector>对。

### 2.3.5 Inverted Index

map函数解析每个文档，并发出一系列<word，document ID>对。 reduce函数接受给定单词的所有对，对相应的文档ID进行排序，并发出<word，list（document ID）>对。 所有输出对的集合形成一个简单的反向索引。 易于扩展此计算以跟踪单词位置。

### 2.3.6 Distributed Sort

map函数从每个记录中提取键，并发出一个<key，record>对。 reduce函数将所有对保持不变。 该计算取决于第4.1节中描述的分区功能和第4.2节中描述的排序属性。

## 3. 实现

MapReduce接口的许多不同实现都是可能的。 正确的选择取决于环境。 例如，一种实现可能适用于小型共享内存计算机，另一种则适用于大型NUMA多处理器，而另一种则适用于网络计算机的更大集合。

本节介绍了针对Google广泛使用的计算环境的实现：通过交换式以太网连接在一起的大型商用PC集群。 在我们的环境中：

1. 机器通常是运行Linux的双处理器x86处理器，每台机器具有2-4 GB的内存。

2. 使用商品网络硬件–在机器级别通常为100MB/S或1GB/S，但是整体对分带宽的平均要小得多。

3. 群集由数百或数千台计算机组成，因此，机器故障很常见。

4. 直接连接到单个计算机的廉价IDE磁盘提供存储。 内部开发的分布式文件系统用于管理存储在这些磁盘上的数据。 该文件系统使用复制在不可靠的硬件上提供可用性和可靠性。

5. 用户将作业提交到调度系统。 每个作业包含一组任务，并由调度程序映射到集群中的一组可用计算机。

### 3.1 执行概述

通过将输入数据自动划分为一组M个拆分，Map调用分布在多台计算机上。 输入拆分可由不同的机器并行处理。 通过使用分区函数（例如，hash（key）mod R）将中间键空间划分为R个片段，可以分布Reduce调用。 分区数（R）和分区功能由用户指定。

![Execution overview](./doc.img/Execution.overview.png)

上图显示了我们实现中MapReduce操作的整体效果。 当用户程序调用MapReduce函数时，将发生以下操作序列（上图中的编号标签与下面列表中的数字相对应）：

1. 用户程序中的MapReduce库首先将输入文件拆分为M个片段，每个片段通常为16MB至64MB（可由用户通过可选参数控制）。然后，它在计算机群集上启动该程序的许多副本。

2. 该程序的副本之一是特殊的-master。 其余的是由master分配工作的worker。 要分配M个Map任务和R个Reduce任务。 Master选择空闲的worker，并为每个worker分配一个map任务或一个reduce任务。

3. 分配了map任务的worker将读取相应输入拆分的内容。 它从输入数据中解析键/值对，并将每对传递给用户定义的Map函数。 由Map函数产生的中间键/值对被缓存在内存中。

4. 缓冲对将定期写入本地磁盘，并通过分区函数划分为R个区域。 这些缓冲对的位置在本地磁盘上被传递回master，该master负责将这些位置转发给reduce worker。

5. 当主服务器通知reduce worker这些位置时，它将使用远程过程调用从map worker的本地磁盘读取缓冲的数据。 当reduce worker读取了所有中间数据时，它将按中间键对数据进行排序，以便将同一键的所有出现都分组在一起。 之所以需要排序，是因为通常有许多不同的键映射到相同的reduce任务。 如果中间数据量太大而无法容纳在内存中，则使用外部排序。

6. Reduce worker对排序的中间数据进行迭代，对于遇到的每个唯一的中间键，它将键和相应的中间值集传递给用户的Reduce函数。 Reduce函数的输出将附加到此reduce分区的最终输出文件中。

7. 完成所有map任务和reduce任务后，master将唤醒用户程序。 此时，用户程序中的MapReduce调用返回到用户代码。

成功完成后，可在R输出文件中使用mapreduce执行的输出（每个归约任务一个，其文件名由用户指定）。 通常，用户不需要将这些R输出文件组合到一个文件中–他们通常将这些文件作为输入传递到另一个MapReduce调用，或者从另一个能够处理被分成多个文件的输入的分布式应用程序中使用它们。

### 3.2. Master Data Structures

Master保留几个数据结构。 对于每个map任务和reduce任务，它存储状态（空闲，进行中或已完成）和worker的标识（对于非空闲任务）。

Master是将中间文件区域的位置从Map任务传播到Reduce任务的管道。 因此，对于每个完成的Map任务，master存储由Map任务生成的R个中间文件区域的位置和大小。 Map任务完成后，将接收到此位置和大小信息的更新。 信息将逐步推送给正在进行Reduce任务的worker。

### 3.3. Fault Tolerance

由于MapReduce库旨在帮助使用数百或数千台计算机处理大量数据，因此该库必须能够容忍机器故障。

#### 3.3.1 Worker Failure

Master定期对每个worker执行ping操作。 如果在一定时间内没有收到来自worker的响应，则master将worker标记为失败。 由worker完成的所有Map任务都将重置为其初始空闲状态，因此有资格在其他worker上进行调度。 同样，在失败的worker上进行的任何Map任务或Reduce任务也将重置为空闲，并有资格进行重新计划。

完成的Map任务在发生故障时会重新执行，因为它们的输出存储在故障机器的本地磁盘上，因此无法访问。 已完成的Reduce任务的输出存储在全局文件系统中，因此无需重新执行。

当先由workerA执行map任务，然后再由workerB执行map任务（因为A失败）时，将向所有执行reduce任务的worker通知重新执行。 任何尚未从workerA读取数据的reduce任务都将从workerB读取数据。

MapReduce可以抵抗大规模的worker故障。 例如，在一次MapReduce操作期间，正在运行的集群上的网络维护导致几分钟内无法访问80个计算机的组。 MapReduce Master只是简单地重新执行了无法访问的worker所完成的工作，并继续取得进展，最终完成了MapReduce操作。

#### 3.3.2 Master Failure

很容易使master写入上述master数据结构的定期检查点。 如果master任务死亡，则可以从最后一个检查点状态开始新副本。 但是，由于只有一个master，因此发生故障的可能性不大。 因此，如果master失败，我们当前的实现将中止MapReduce计算。 客户可以检查这种情况，并根据需要重试MapReduce操作。

#### 3.3.3 Semantics in the Presence of Failures

当用户提供的map和reduce运算符是其输入值的确定性函数时，我们的分布式实现将产生与整个程序的无故障顺序执行相同的输出。

我们依靠Map和Reduce任务输出的原子提交来实现此属性。 每个正在进行的任务都将其输出写入私有临时文件。 Reduce任务产生一个这样的文件，而Map任务产生R个这样的文件（每个Reduce任务一个）。 Map任务完成后，worker会向master发送一条消息，并在消息中包含R个临时文件的名称。 如果master收到有关已完成的map任务的完成消息，它将忽略该消息。 否则，它将在master数据结构中记录R文件的名称。

当reduce任务完成时，reduce worker自动将其临时输出文件重命名为最终输出文件。 如果在多台计算机上执行相同的reduce任务，则将对同一最终输出文件执行多个重命名调用。 我们依赖于底层文件系统提供的原子重命名操作，以确保最终文件系统状态仅包含一次执行reduce任务所产生的数据。

我们的map和reduce运算符绝大多数是确定性的，在这种情况下我们的语义等同于顺序执行的事实使程序员很容易就其程序行为进行推理。 当map and/or reduce运算符不确定时，我们提供较弱但仍然合理的语义。 在存在不确定性运算符的情况下，特定reduce任务R1的输出等效于由不确定性程序的顺序执行产生的R1的输出。 然而，用于不同reduce任务R2的输出可以对应于由不确定性程序的不同顺序执行所产生的用于R2的输出。

考虑map任务M和reduce任务R1和R2。 令e（Ri）为已落实的Ri的执行（恰好有一个这样的执行）。 之所以出现较弱的语义，是因为e（R1）可能已经读取了一次执行M所产生的输出，而e（R2）可能已经读取了执行不同的M所产生的输出。

### 3.4. Locality

在我们的计算环境中，网络带宽是相对稀缺的资源。 我们利用输入数据（由GFS管理）存储在组成集群的计算机的本地磁盘上的事实，从而节省了网络带宽。 GFS将每个文件分成64 MB的块，并在不同的计算机上存储每个块的几个副本（通常为3个副本）。 MapReduce Master将输入文件的位置信息考虑在内，并尝试在包含相应输入数据副本的计算机上计划Map任务。 如果失败，它将尝试在该任务的输入数据的副本附近计划Map任务（例如，在与包含数据的计算机位于同一网络交换机的worker计算机上）。 在集群中很大一部分的工作线程上运行大型MapReduce操作时，大多数输入数据都在本地读取，并且不占用网络带宽。

### 3.5. Task Granularity

如上所述，我们将Map阶段细分为M个片段，将Reduce阶段细分为R个片段。 理想情况下，M和R应该比工作计算机的数量大得多。 让每个worker执行许多不同的任务可以改善动态负载平衡，并且还可以在worker失败时加快恢复速度：它完成的许多Map任务可以分布在所有其他worker计算机上。

如上所述，由于master必须制定O（M + R）调度决策并将O（M \* R）状态保留在内存中，因此在我们的实现中可以有多大的M和R有实际的界限。 （但是，内存使用量的常量因素很小：状态的O（M \* R）块每个Map任务/Reduce任务对大约包含一个字节的数据。）

此外，R通常受用户约束，因为每个reduce任务的输出最终都存储在单独的输出文件中。 在实践中，我们倾向于选择M，以便每个单独的任务大约是16 MB到64 MB的输入数据（这样，上述的位置优化是最有效的），并且我们将R设为我们worker计算机数量的一小部分期望使用。 我们经常使用2,000个worker执行M = 200,000和R = 5,000的MapReduce计算。

### 3.6. Backup Tasks

延长MapReduce操作总时间的常见原因之一是Straggler：一台机器花费异常长的时间来完成最后几个Map之一或Reduce计算任务。 Straggler之所以出现，可能是由于多种原因。 例如，磁盘损坏的计算机可能会经常遇到可纠正的错误，从而将其读取性能从30 MB / s降低到1 MB / s。 集群调度系统可能已在计算机上调度了其他任务，由于竞争CPU，内存，本地磁盘或网络带宽，导致其执行MapReduce代码的速度较慢。 我们最近遇到的一个问题是机器初始化代码中的一个错误，该错误导致了处理器缓存被禁用：受影响机器上的计算速度降低了100倍。

我们有一个通用的机制来缓解Straggler的问题。 当MapReduce操作接近完成时，Master会调度其余正在进行的任务的备份执行。 每当主执行或备份执行完成时，该任务就会标记为已完成。 我们已经对该机制进行了调整，以使其通常将操作使用的计算资源增加不超过百分之几。 我们发现，这大大减少了完成大型MapReduce操作的时间。 例如，禁用备份任务机制时，第5.3节中描述的排序程序需要延长44％的时间才能完成。
