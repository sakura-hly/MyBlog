# The Google File System

**关键字**：容错，扩展性，数据存储，集群存储

## 1. 介绍

我们已经设计并实现了Google File System(GFS)来应对Google在数据处理方面快速增长的需求。GFS有着和以前的分布式文件系统共同的目标，比如性能，扩展性，可靠性和可用性。然而，其设计一直由对我们当前和预期的应用程序负载和技术氛围的观察所驱动，这也反映出与一些早期文件系统设计理念有差异。

**第一，组件的失败并非是异常，而是正常的**。这个文件系统由成百上千个廉价的商品部件拼装的存储机器组成，并且被相当数量的客户端机器访问。组件的数量和质量实际上保证了某些组件在任何给定的时间都无法运行，并且某些组件将无法从当前的失败中恢复过来。我们已经见过由下列因素造成的问题：程序bug，操作系统bug，人类犯错，以及磁盘，内存，连接器，网络，电源的失败。所以，不间断的监控，错误侦测，错误容忍，以及自动恢复这些功能必定要集成到系统中。

**第二，按照传统标准文件是巨大的**。几GB的文件很常见。每个文件通常包含很多应用程序对象，比如web文档。当我们定期处理包含数十亿对象的许多TBs的快速增长的数据集，即使文件系统支持数十亿约KB大小的文件，也难以管理。总之，必须重新考虑设计理念以及一些参数，比如I/O操作和块大小。

**第三，大部分文件的修改是追加新数据而不是覆写原数据**。实际中，文件的随机写是不存在的。文件被写入后只用来读，而且经常是顺序读。各种数据都拥有这些特征。一些可能会组成大型存储库，被数据分析程序扫描。一些可能是应用程序持续生成的数据流。一些可能是归档数据。一些可能是一台机器上产生的中间结果，而在另一台机器上同时或稍后产生。鉴于对大型文件的这种访问方式，追加写成为性能优化和原子性保证的重点，然而把数据块存储在客户端则失去了吸引力。

**第四，共同设计应用程序和文件系统API增加灵活性使得整个系统受益**。例如，我们放宽了GFS的一致性模型，以极大地简化文件系统，并且不会给应用程序带来繁重的负担。我们也介绍了一个原子性的并发操作以至于多个客户端可以同时对一个文件追加而不需要额外的同步。

## 2. 设计概述

### 1. 假设

* 该系统由许多经常发生故障的廉价商品组件组成。它必须不间断地自我监控和检测，并从组件故障中迅速恢复。

* 系统存储少量的大文件。我们预计有几百万个文件，每个通常100MB或者更大。几GB的文件很常见，需要有效地管理。小文件必须支持，但是我们不必对其优化。

* 工作负载主要源于两种读：大量流读取和小量随机读。对于大量流读取，每次通常读取几百KBs，1MB或更多。同一个客户端的后续操作经常读取文件的连续区域。一个小量随机读通常在随机位置读取几KBs。注重性能的应用程序经常对小读取进行批处理和排序，以稳定地通过文件而不是来回移动。

* 高持续带宽比低延迟更重要，我们的大多数目标应用程序都很重视高速处理大量数据，而很少会对单次读写的响应时间有严格要求。

### 2. 接口

GFS提供熟悉的文件系统接口，尽管它没有实现一个标准API比如POSIX。文件在目录中按层次结构组织，并由路径名标识。 我们支持创建，删除，打开，关闭，读取和写入文件的常规操作。

除此之外，GSF有快照和记录追加操作。快照低成本地创建文件或目录树的副本。记录追加允许多个客户端同时将数据追加到同一个文件中，同时保证每个客户端的追加的原子性。这对于实现多路合并结果和生产者消费者队列很有用，许多客户端可以在不加锁的情况下追加。

### 3. 架构

一个GFS集群由一个master和多个chunkservers组成，并且被多个客户端访问。如下图，这些设备通常都是运行用户级服务器进程的商用Linux机器。同一个机器上很容易运行一个chunkserver和一个客户端，只要机器资源允许，并且可以接受运行不稳定的应用程序引起较低的可靠性。

文件被划分到固定大小的chunk。每个chunk被创建时，master会分配一个不可变、全局唯一的64位chunk handle，并以此标识。Chunkservers以Linux文件的形式存储chunks在本地磁盘，并读写由chunk handle和字节范围指定的chunk数据。对于可靠性，每个chunk在多个chunkservers上被复制。默认情况下，我们存储三个副本，尽管用户可以为文件名称空间的不同区域指定不同的复制级别。

Master维护所有的文件系统元数据，包括命名空间，访问控制信息，文件到chunk的映射，和chunks的当前位置。它还控制整个系统的活动，例如chunk的租赁管理，孤立块的垃圾回收以及chunk在chunkservers的迁移。Master周期性地以心跳消息与每个chunkservers通信，下达指令和收集状态。

GFS客户端实现了文件系统API，并与master和chunkservers通信以代表应用程序读写数据。客户端与服务器交互来进行元数据操作，但是所有承载数据的通信都直接发送给chunkservers。

客户端和chunkservers都不缓存文件数据。客户端缓存几乎没有什么好处，因为大多数应用程序会处理巨大的文件，或者工作集太大而无法缓存。 没有它们，就消除了缓存一致性问题，从而简化了客户端和整个系统。 （但是，客户端确实缓存元数据。）块服务器不需要缓存文件数据，因为大块存储为本地文件，因此Linux的缓冲区缓存已经将经常访问的数据保存在内存中。
![GFS Architecture](./doc.img/GFS.Architecture.png)

### 4. Single Master

拥有一个master极大地简化了我们的设计，并使master可以使用全局信息来制定复杂的chunk放置和复制决策。然而， 我们必须最小化它在读写中的参与，以免它成为瓶颈。客户端永远不会通过master读写文件数据。取而代之的是，客户端询问主服务器应该联系哪些块服务器。 它在有限的时间内缓存此信息，并直接与块服务器交互以进行许多后续操作。

我们参考上图简单解释一下读操作的交互，首先，使用固定的chunk大小，客户端将应用程序指定的文件名和字节偏移量转换为文件内的chunk索引。 然后，它向master发送一个包含文件名和chunk索引的请求。 master响应相应的chunk handle和副本的位置。 客户端使用文件名和chunk索引作为关键字来缓存此信息。

然后，客户端将请求发送到其中一个副本，很可能是最接近的副本。 该请求指定了chunk handle和该chunk内的字节范围。 在缓存信息过期或重新打开文件之前，对同一chunk的进一步读取不再需要客户端与master之间的交互。 实际上，客户端通常会在同一请求中请求多个chunk，而master也可以在请求的chunk之后立即包含chunk的信息。 这些额外的信息绕开了未来几个客户-master交互的机会，而实际上没有任何额外的开销。

### 5. Chunk Size

Chunk size是一个关键的设计参数。我们选择了64MB，比通常的文件系统block size大得多。每个chunk副本作为纯Linux文本存储在chunkserver，并且在需要时进行扩展。Lazy空间分配避免了由于内部碎片而浪费空间。

大chunk size具有几个重要的优点。第一，它减少了客户端与master的交互需求，因为对同一个chunk的读写只需要向master发生一个初始请求即可获取chunk位置信息。这对于我们的工作量尤其重要，因为应用程序通常顺序读写大文件。即使是小的随机读，客户端也可以轻松地缓存多个TB工作集的所有chunk位置信息。其次，由于在较大的块上，客户端更有可能在给定的块上执行许多操作，因此它可以通过在延长的时间段内保持与块服务器的持久TCP连接来减少网络开销。第三，它减小了存储在主服务器上的元数据的大小。 这使我们能够将元数据保留在内存中，从而带来其他优势，

另一方面，大chunk size（即使具有惰性空间分配）也有其缺点。 一个小文件由少量chunk组成，也许只有一个。 如果许多客户端正在访问同一文件，则存储这些chunks的chunkservers可能会成为热点。 实际上，热点并不是主要问题，因为我们的应用程序通常会顺序读取大型的多chunks文件。

但是，当批处理队列系统首次使用GFS时，热点确实出现了：可执行文件作为一个单chunk文件写入GFS，然后同时在数百台计算机上启动。 数以百计的同时请求使存储此可执行文件的少数块服务器超负荷运行。 我们通过存储具有较高复制因子的可执行文件并通过使批处理队列系统错开应用程序的启动时间来解决此问题。 潜在的长期解决方案是在这种情况下允许客户端从其他客户端读取数据。

### 5. Metadata

Master存储三种主要类型的元数据：文件和chunk名称空间，从文件到chunk的映射以及每个chunk副本的位置。所有元数据都保存在master的内存中，前两种类型（命名空间和文件到chunk的映射）还通过将修改记录到存储在master本地磁盘上并在远程计算机上复制的操作日志中而保持不变。使用日志可以使我们简单、可靠地更新master状态，而不会在master崩溃时冒不一致的风险。master服务器不会永久存储块位置信息，相反，它会在启动时以及每当一个chunk服务器加入集群时就向每个chunk服务器询问其chunk。
